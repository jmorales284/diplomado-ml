# -*- coding: utf-8 -*-
"""taller1V2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hGq4XxwjdG7LycSgaucSQorm0SPJynAk

#NOMBRES
###Juan David Portilla Morales
###Brayan Andres Erazo Erazo

###Importaciones
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import seaborn as sns
#Importamos el conjunto de datos de scikit-learn Optical recognition of handwritten digits
from sklearn import datasets

"""Explorar los datos (ver Taller 1 V1),"""

# Cargar el conjunto de datos
digits = datasets.load_digits()

# Número de imágenes que quieres mostrar (ajusta según sea necesario)
num_images_to_show = 15

# Obtener índices aleatorios
random_indices = np.random.choice(len(digits.images), num_images_to_show, replace=False)

# Crear la figura y subgráficos
fig, axes = plt.subplots(3, 5, figsize=(10, 6))

# Iterar sobre las imágenes y etiquetas con los índices aleatorios
for i, idx in enumerate(random_indices):
    image, label = digits.images[idx], digits.target[idx]
    axes[i // 5, i % 5].imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
    axes[i // 5, i % 5].set_title(f'Target: {label}')

# Ajustar el diseño y mostrar la figura
plt.tight_layout()
plt.show()

from google.colab import drive
drive.mount('/content/drive')

"""2. Realizar una reducción de dimensión sobre el conjunto de datos e identificar a cuanto se puede reducir sin perder tanta información (>70%) (Utilizar PCA)."""

# Separar los datos en características (X) y etiquetas (y)
X = digits.data
y = digits.target

X.shape, y.shape

#Normalizamos los datos, Es decir escalamos los valores a un rango de 0 a 1 para facilitar el PCA
X_normalized = X / 16  #16 es el valor maximo de pixeles del dataset
X_normalized

pca = PCA()
pca.fit(X_normalized)

explained_variance_ratio = pca.explained_variance_ratio_
#creamos la varianza acumulativa y la sumamos para saber que dimensionalidad podemos agarrar para que no se pierdan tantos datos, con cumsum hacemos la suma acumulativa del array
cumulative_variance_ratio = np.cumsum(explained_variance_ratio)
explained_variance_ratio[0]

# Encontrar el número óptimo de componentes para superar el umbral del 70%
n_components_70 = np.argmax(cumulative_variance_ratio > 0.70) + 1
n_components_70

# Visualizar la varianza acumulativa explicada por cada componente principal, aqui podemos ver graficamente el numero optimo de componentes para superar el umbral del 70%
plt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, marker='o')
plt.xlabel('Número de Componentes Principales')
plt.ylabel('Varianza Acumulativa Explicada')
plt.title('Varianza Acumulativa Explicada por PCA')
plt.show()

#realizamos la reduccion de dimensionalidad con el numero optimo
pca = PCA(n_components = n_components_70)
X_pca = pca.fit_transform(X_normalized)
X_pca.shape

"""3. Clasificar los datos reducidos y sin reducir y mostrar cuál es el score y tiempo de entrenamiento"""

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
import time

# Separar los datos para clasificación, en datos de entrenamiento y prueba para datos originales y reducidos por pca
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #datos originales
X_train_pca, X_test_pca, _, _ = train_test_split(X_pca, y, test_size=0.2, random_state=42) #datos con pca

# Clasificador con DecisionTreeClassifier y datos originales
dt_classifier_original = DecisionTreeClassifier()
start_time_original = time.time()
dt_classifier_original.fit(X_train, y_train)
end_time_original = time.time()
y_pred_original = dt_classifier_original.predict(X_test)
accuracy_original = accuracy_score(y_test, y_pred_original)

# Imprimir resultados para datos originales
print("Resultados con DecisionTreeClassifier y datos originales:")
print("Precisión:", accuracy_original)
print("Tiempo de entrenamiento:", end_time_original - start_time_original)

# Clasificador con DecisionTreeClassifier y datos reducidos por PCA
dt_classifier_pca = DecisionTreeClassifier()
start_time_pca = time.time()
dt_classifier_pca.fit(X_train_pca, y_train)
end_time_pca = time.time()
y_pred_pca = dt_classifier_pca.predict(X_test_pca)
accuracy_pca = accuracy_score(y_test, y_pred_pca)

# Imprimir resultados para datos reducidos por PCA
print("\nResultados con DecisionTreeClassifier y datos reducidos por PCA:")
print("Precisión:", accuracy_pca)
print("Tiempo de entrenamiento:", end_time_pca - start_time_pca)

"""4. Agrupar los datos reducidos con el número de cluster óptimo"""

from sklearn.cluster import KMeans
from sklearn import metrics

wcss = []
scss = []

# Iterar sobre diferentes números de clusters
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, random_state=2023)
    kmeans.fit(X_pca[:, :n_components_70])  # Utilizamos solo las primeras n_components_70 componentes principales

    if i > 1:
        labels = kmeans.labels_
        scss.append(metrics.silhouette_score(X_pca[:, :n_components_70], labels))
    wcss.append(kmeans.inertia_)

# Dibujar los resultados para el método del codo
plt.plot(range(1, 11), wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')  # Suma de cuadrados de grupo (inertia_)
plt.show()

# Dibujar los resultados para el método de la silueta
plt.plot(range(2, 11), scss)
plt.title('The Silhouette Method')
plt.xlabel('Number of Clusters')
plt.ylabel('Silhouette Score')
plt.show()

"""Podemos decir que el numero de cluster optimo es 3"""

# Aplicar K-Means con el número óptimo de clusters
kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300, random_state=2023)
kmeans.fit(X_pca)

# Predecir los clusters para cada punto de datos
ykm = kmeans.predict(X_pca)
kmeans.inertia_,kmeans.score(X_pca)

kmeans.cluster_centers_

# Visualizar los clusters
plt.subplot(1, 2, 1)
plt.scatter(X_pca[ykm == 0, 0], X_pca[ykm == 0, 1], c='cyan')
plt.scatter(X_pca[ykm == 1, 0], X_pca[ykm == 1, 1], c='blue')
plt.scatter(X_pca[ykm == 2, 0], X_pca[ykm == 2, 1], c='yellow')
# Se dibujan los centroides
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=100, c='red', label='Centroids')

plt.subplot(1, 2, 2)
plt.scatter(X_pca[y == 1, 0], X_pca[y == 1, 1], marker='o', c='yellow')
plt.scatter(X_pca[y == 2, 0], X_pca[y == 2, 1], marker='o', c='blue')
plt.scatter(X_pca[y == 3, 0], X_pca[y == 3, 1], marker='o', c='cyan')
plt.show()

"""5. Mostrar el area de clasificación del modelo de clustering y de clasificación (con datos reducidos) y escribir analiticamente que tanto se parecen las clasificaciones de los dos modelos"""

# Visualizar el área de clasificación del modelo de clustering (KMeans)
plt.subplot(1, 3, 2)
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=ykm, cmap='viridis', edgecolors='k')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=100, c='red', marker='X', label='Centroids')
plt.title('Modelo de Clustering')
plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
plt.legend()

# Visualizar el área de clasificación del modelo con Decision Tree Classifier
plt.subplot(1, 3, 3)
plt.scatter(X_test_pca[:, 0], X_test_pca[:, 1], c=y_pred_pca, cmap='viridis', edgecolors='k')
plt.title('Decision Tree Classifier con PCA')
plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')

plt.tight_layout()
plt.show()

"""#Analisis:

Observando ambas clasificaciones, se puede concluir que mediante el modelo clustering los datos son más claros y precisos, tambien se puede apreciar que no se pierde tanta informacion lo cual si es muy notorio en el otro metodo de clasificacion mediante datos reducidos.

"""